{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f10feeb1-9bff-4086-9d5a-41c3da1a4dbd",
   "metadata": {},
   "source": [
    "# Proccess CTD data\n",
    "<img src=\"CDTVikingPenguin.webp\" alt=\"Notebook Mascot\" width=\"400\"/>\n",
    "\n",
    "## HOWTO\n",
    "The cell below will create ODV compatiable data files for each vp2 file in the specified directory.\n",
    "To execute the cell press the play button in the menu above or select the cell and use ``CTRL+Enter``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "283b2f2a-89f4-44c4-bf1e-b8368f5c8118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving data: ./VL_92775_250209114751_ODV.csv\n",
      "Saving data: ./VL_92775_250209114751 Duplicate_ODV.csv\n",
      "Saving data: ./XPO250205_all_ODV.csv\n",
      "Summary copied to clipboard\n",
      "All Done!\n"
     ]
    }
   ],
   "source": [
    "#######################\n",
    "### Input Variables ###\n",
    "#######################\n",
    "\n",
    "# Variables that change each time\n",
    "voyage_id = 'XPO250205'\n",
    "voyage_type = 'Antarctic Explorer'\n",
    "input_dir = './'\n",
    "output_dir = './' # If None then input_dir is used\n",
    "\n",
    "# Names of the file created. These files will be placed in the output_dir\n",
    "# These are not f-strings as data_filename will be different for each file.\n",
    "# You can use the following placeholders to define the file names\n",
    "# ``{filename}`` will insert the name of the vp2 file (minus the extension)\n",
    "# ``{voyage_id}`` will insert the voyage id given above\n",
    "data_filename = '{filename}_ODV.csv'\n",
    "all_data_filename = '{voyage_id}_all_ODV.csv'\n",
    "\n",
    "# Data in the HEADER section to add to each row in the DATA section\n",
    "# There needs to be a unit string for each HEADER item. Use empty string to skip units\n",
    "metadata_to_include = ['Latitude', 'Longitude']\n",
    "metadata_units = ['DD', 'DD']\n",
    "\n",
    "#################\n",
    "### Main Code ###\n",
    "#################\n",
    "# This could be put in a python file and called from the notebook\n",
    "# This means if something happens to the notebook all the code isnt lost ;)\n",
    "\n",
    "import csv, os\n",
    "import pyperclip\n",
    "\n",
    "month_names = {'01': 'Jan', '02': 'Feb', '03': 'Mar', '04': 'Apr', '05': 'May', '06': 'Jun', \n",
    "               '07': 'Jul', '08': 'Aug', '09': 'Sep', '10': 'Oct', '11': 'Nov', '12': 'Dec'}\n",
    "\n",
    "def save_data(data, filepath):\n",
    "    print(f'Saving data: {filepath}')\n",
    "    nrows = {len(v) for v in data.values()}\n",
    "    if len(nrows) != 1:\n",
    "        raise ValueError('Not all columns have the same number of rows')\n",
    "    else:\n",
    "        nrows = nrows.pop()\n",
    "\n",
    "    with open(filepath, 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file, dialect='excel') \n",
    "\n",
    "        # Writes the column labels\n",
    "        writer.writerow(list(data.keys()))\n",
    "\n",
    "        # Writes the data\n",
    "        for i in range(nrows):\n",
    "           writer.writerow([data[k][i] for k in data.keys()])\n",
    "        \n",
    "\n",
    "def process_vp2(voyage_id, voyage_type, \n",
    "                input_dir, output_dir, \n",
    "                data_filename, all_data_filename, \n",
    "                metadata_to_include, metadata_units):\n",
    "    # Make sure varables are correct\n",
    "    # data_filename and all_data_filenames should end with csv\n",
    "    if not data_filename.endswith('.csv'):\n",
    "        data_filename += '.csv'\n",
    "    \n",
    "    if not all_data_filename.endswith('.csv'):\n",
    "        all_data_filename += '.csv'\n",
    "    \n",
    "    # Directory paths should end with a slash\n",
    "    if not input_dir.endswith('/'):\n",
    "        input_dir = input_dir + '/'\n",
    "    \n",
    "    if output_dir is None: \n",
    "        output_dir = input_dir\n",
    "    elif not output_dir.endswith('/'):\n",
    "        output_dir = output_dir + '/'\n",
    "    \n",
    "    # A list of (filename, filepath) tuples made up of all vp2 files in the specified directory\n",
    "    vp2files = [(file.removesuffix('.vp2'), f\"{input_dir}{file}\") for file in os.listdir(input_dir) if file.endswith('.vp2')]\n",
    "    \n",
    "    # We define these here as they are shared amongst all files\n",
    "    all_data = {}\n",
    "    summary = []\n",
    "    all_data_size = 0\n",
    "    \n",
    "    # Iterate though all files\n",
    "    for filename, filepath in vp2files:\n",
    "        with open(filepath, 'r') as csv_file:\n",
    "            reader = csv.reader(csv_file, delimiter='\\t')\n",
    "    \n",
    "            # This reads the HEADER section\n",
    "            header_metadata = {} # We define this here so its reset for everyfile\n",
    "            for header_row in reader:\n",
    "                if len(header_row) >= 1 and header_row[0] == '[HEADER]':\n",
    "                    for row in reader:\n",
    "                        if len(row) == 0 or row[0] == '':\n",
    "                            break # end of section\n",
    "                        else:\n",
    "                            k, v = row[0].split('=', 1)\n",
    "                            header_metadata[k] = v\n",
    "    \n",
    "                # This reads the DATA section\n",
    "                if len(header_row) >= 1 and header_row[0] == '[DATA]':\n",
    "                    column_titles = [c.strip() for c in next(reader)] # First row is the column titles\n",
    "                    column_units = [c.strip() for c in next(reader)] # Second row is the column units\n",
    "    \n",
    "                    # Here we create a column label that consists of the title and unit, if there is one\n",
    "                    column_labels = column_titles[:] # creates a copy of the list\n",
    "                    for i, unit in enumerate(column_units):\n",
    "                        if unit != '':\n",
    "                            column_labels[i] += f' [{unit}]'\n",
    "                    \n",
    "                    # Create a empty dictionary to store the data in\n",
    "                    data = {label: [] for label in column_labels}\n",
    "    \n",
    "                    # Add the metadata columns we want to include\n",
    "                    metadata_labels = metadata_to_include[:]\n",
    "                    for c, metadata_column in enumerate(metadata_to_include):\n",
    "                        # Add unit to labels\n",
    "                        if metadata_units[c] != '':\n",
    "                            metadata_labels[c] += f' [{metadata_units[c]}]'\n",
    "                        \n",
    "                        \n",
    "                        data[metadata_labels[c]] = []\n",
    "    \n",
    "                    # Here we read all the data, row by row\n",
    "                    number_of_rows = 0\n",
    "                    for row in reader:\n",
    "                        if len(row) == 0 or row[0] == '':\n",
    "                            break # end of section\n",
    "                        else:\n",
    "                            number_of_rows += 1\n",
    "                            # Read all the values in the row and add it to the corresponding column\n",
    "                            for c, title in enumerate(column_titles):\n",
    "                                if c < len(row):\n",
    "                                    data[column_labels[c]].append(row[c].strip())\n",
    "                                else:\n",
    "                                    # In case there is data missing we dont get an error\n",
    "                                    data[column_labels[c]].append('')\n",
    "    \n",
    "                            # The metadata is constant so we just add the static value to each row\n",
    "                            for c, metadata_column in enumerate(metadata_to_include):\n",
    "                                data[metadata_labels[c]].append(header_metadata.get(metadata_column, ''))\n",
    "                                \n",
    "            # Finished reading the file\n",
    "            \n",
    "            # Save data to file\n",
    "            output_filepath = output_dir+data_filename.format(voyage_id = voyage_id, filename=filename)\n",
    "            save_data(data, output_filepath)\n",
    "            \n",
    "    \n",
    "            # Update all_data with data from this file\n",
    "            for label in data:\n",
    "                if label not in all_data:\n",
    "                    # If label doesnt exist in all_data then add it and fill it with empty values\n",
    "                    # This is incase the columns are not the same in all files\n",
    "                    all_data[label] = ['' for i in range(all_data_size)]\n",
    "                \n",
    "                all_data[label].extend(data[label])\n",
    "    \n",
    "            # Again this is a precaution in case the columns are not the same in all files\n",
    "            # Makes sure that the size of everything in all_data is the same\n",
    "            for label in all_data:\n",
    "                if label not in data:\n",
    "                    all_data[label].extend(['' for i in range(number_of_rows)])\n",
    "    \n",
    "            all_data_size += number_of_rows\n",
    "\n",
    "            # Parse date\n",
    "            date, time = header_metadata.get('DateStartTime', 'YYYY/MM/DD H:M:S').split()\n",
    "            year, month, day = date.split('/')\n",
    "            month = month_names.get(month, month)\n",
    "            \n",
    "            # Update summary\n",
    "            summary.append([])\n",
    "            summary[-1].append('') # Benthic Photo Name\n",
    "            summary[-1].append(filename) # Filename\n",
    "            summary[-1].append(f'{voyage_id} - {voyage_type}') # Voyage Name\n",
    "            summary[-1].append(f'{year}-{month}-{day}') # Date (YEAR-MONTH-DAY)\n",
    "            summary[-1].append(time) # Time (UTC)\n",
    "            summary[-1].append('') # Location\n",
    "            summary[-1].append('') # Dive Number\n",
    "            summary[-1].append(header_metadata.get('Latitude', '')) # Latitude (DD)\n",
    "            summary[-1].append(header_metadata.get('Longitude', '')) # Longitude (DD)\n",
    "            summary[-1].append(header_metadata.get('MaxDepthM', '')) # Maximum Depth (m)\n",
    "            summary[-1].append('') # Mode\n",
    "            summary[-1].append('') # Comments\n",
    "\n",
    "    # Save all_data to file\n",
    "    output_filepath = output_dir+all_data_filename.format(voyage_id = voyage_id, filename='filename')\n",
    "    save_data(all_data, output_filepath)\n",
    "    \n",
    "    # copy summary to clipboard\n",
    "    clipboard_text = '\\n'.join(['\\t'.join(summary[i]) for i in range(len(summary))])\n",
    "    pyperclip.copy(clipboard_text)\n",
    "    print('Summary copied to clipboard')\n",
    "    print('All Done!')\n",
    "\n",
    "###############\n",
    "### Execute ###\n",
    "###############\n",
    "\n",
    "# If you place the code above in a seperate file then uncomment the following line\n",
    "# from <pythonfilename> import process_vp2\n",
    "\n",
    "process_vp2(voyage_id, voyage_type, \n",
    "            input_dir, output_dir, \n",
    "            data_filename, all_data_filename, \n",
    "            metadata_to_include, metadata_units)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
